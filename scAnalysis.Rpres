```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
```

scAnalysis
========================================================
author: Timothy Tickle and Brian Haas
css: 2014_scAnalysis.css
date: September 23, 2014

Logistics
===

```{r, tidy=TRUE}
library(caret) #Near-zero filter
library(GMD) #Cluster selection on dendrograms
library(gplots) #Colorpanel
library(mclust) # Selection of clusters
library(scde) #Statistical inference (bayesian mixed model)
library(vegan) #PCoA, distance metrics
source("heatmap.3b.R")
source("cell_cycle_plot.R")
source("Modules.R")
# RcppArmadillo, flexmix, multicore, Cairo
```

Today's Data Set
===

- Describe data set

How do we get it?
===

```{r}
# Load tab delimited file
data = read.delim( "data/GSE29087_L139_expression_tab.txt", row.names = 1 )

# For convenience splitting the data frame in to metadata and data
metadata = data[ 1:6 ]
data = data[ -1 * 1:6 ]

# Remove features without counts
zero.features <- which( apply( data, 1, sum ) == 0 )
length( zero.features )
data = data[ -1 * zero.features, ]
metadata = metadata[ -1 * zero.features, ]
```

Always look at your data
===

# Add picture

A quick look at the data
===

```{r}
dim( data )
names( data )
summary(data)

dim( metadata )
names( metadata )
summary(metadata)
```

Let's characterize this data.
===

- Is the data normal?
- Sparsity / zero-inflation
- Overdispersed

Normality?
===

```{r}
feature.sum = apply( data, 1, sum )
feature.sum.sorted = sort( feature.sum )
plot( log( feature.sum.sorted ), main = "Total gene counts throughout samples", ylab = "Log total counts", xlab = "Index after sorting" )
abline( v = c(1, 3729,7457,11180,14913), col = c("red", "violet","cyan","blue","green"))
# Min 1, 1st quartile 3729, Median 7457, 3rd quartile 11180, Max 14913
```

Normality?
===

```{r}
feature.sum.order <- order( feature.sum )
index_min <- feature.sum.order[ 1:10 ]
index_q1 <- feature.sum.order[ 3724:3723 ]
index_median <- feature.sum.order[ 7452:7461 ]
index_q3 <- feature.sum.order[ 11175:11184 ]
index_max <- feature.sum.order[ 14908:14913 ]
plot( density( as.matrix( data[ index_min[1], ] ) ), col = "red" )
```

Zooming in to Gene Distributions
===
```{r, echo = FALSE, fig.keep='last'}
plot( x=0,y=0,type="p", xlim=c(0,100), ylim=c(0,.2) )
for( i_min_plot in index_min ){ lines( density( as.matrix( data[ i_min_plot, ])), col = "red", add = TRUE)}
for( i_q1_plot in index_q1 ){ lines( density( as.matrix( data[ i_q1_plot, ])), col = "violet", add = TRUE)}
for( i_median_plot in index_median ){ lines( density( as.matrix( data[ i_median_plot, ])), col = "cyan", add = TRUE)}
for( i_q3_plot in index_q3 ){ lines( density( as.matrix( data[ i_q3_plot, ])), col = "blue", add = TRUE)}
for( i_max_plot in index_max ){ lines( density( as.matrix( data[ i_max_plot, ])), col = "green", add = TRUE)}
```

Sparsity
===

```{r}
# Check samples for read depth
sample.depth <- apply( data, 2, sum )
summary(sample.depth )
```
---
```{r}
barplot( sort( sample.depth ), main = "Sample Depth ", xlab = "Sample", ylab = "Depth")
```

Sparsity
===

```{r}
# Percent zero by feature expression
feature.percent.zero <- apply( data, 1, function(x){ length( which(x==0))/length(x)})
feature.mean.no.zero <- apply( data, 1, function(x){ mean( x[ which(x!=0) ] ) })
plot( feature.percent.zero, log( feature.mean.no.zero ), xlab = "log mean", ylab = "Percent Zero", main = "Feature Sparsity by Expression" )
```

Overdispersion
===
```{r}
# SD vs Mean
feature.sd.with.zero <- apply( data, 1, sd )
feature.mean.with.zero <- apply( data, 1, mean )
feature.sd.no.zero <- apply( data, 1, function(x){ sd( x[ which(x !=0 )])})
plot( log( feature.mean.no.zero ), log( feature.sd.no.zero ), xlab = "Mean (Log)", ylab = "SD (Log)", main = "SD vs mean (ignoring zeros)" )
plot( log( feature.mean.with.zero ), log( feature.sd.with.zero ), xlab = "Mean (Log)", ylab = "SD (Log)", main = "SD vs mean (ignoring zeros)" )
```

Can QC Help?
===

- Removing very sparse features
- Imputing outliers

Removing Sparse Features
===

```{r}
sample.percentile = apply( data, 2, function(x){ quantile(x[x !=0 ], .5)})
feature.noise = which(apply( data, 1, func_min_occurence_at_min_value, sample.percentile ) <= 10)
feature.noise.by.expression = order( apply( data[ feature.noise, ], 1, sum ), decreasing = TRUE)
# What am I removing
plot(density( as.matrix(data[ feature.noise[ feature.noise.by.expression[ 1 ]], ])))
# TODO make a multiple density plot
```

Removing Sparse Features
===

```{r}
dim( data )
dim( metadata )
data = data[ -1 * feature.noise, ]
metadata = metadata[ -1 * feature.noise ]
dim( data )
dim( metadata )
```

Sample Read Depth: revisited
===

```{r}
sample.depth <- apply( data, 2, sum )
depth.colors = polychromatic_palette( length( sample.depth ) )
sample.depth
```
---
```{r}
barplot( sort(apply( data, 2, sum )))
```

Can Transforms / Normalization Help?
===

Challenge
===

- We have a large range of read depth per sample
  - Is this a problem?

Waste Not, Want Not
===

- Rarefy?

Dimensionality Reduction and Ordination
===

- A frequently used method is PCA

PCA: in quick theory
===

- Describe PCA

PCA: in practice
===

- Things to be aware of

PCA: in code
===

```{r, echo=TRUE}
# Row center and log
data.scaled = t( scale( t( as.matrix( log( data + 1 ) ) ), center=TRUE, scale=TRUE ) )
# Remove constant rows
#TODO data.scaled = data.scaled[, -1 * nearZeroVar( data.scaled ) ]
# Perfrom PCA
results.pca = prcomp( data.scaled, retx = TRUE )
```

PCA: in code
===

```{r, echo=TRUE}
plot( results.pca$rotation[,1], results.pca$rotation[,2], pch=16 )
```

PCA: by depth
===
```{r,echo=FALSE}
min.pc1 = min( results.pca$rotation[,1] )
min.index = which(sort(results.pca$rotation[,1]) == min.pc1 )[1]
min.pc1 = round(min.pc1,4)
median.pc1 = median(c(0,results.pca$rotation[,1]))
median.index = which(sort(results.pca$rotation[,1]) == median.pc1 )[1]
median.pc1 = round(median.pc1,4)
max.pc1 = max( results.pca$rotation[,1] )
max.index = which(sort(results.pca$rotation[,1]) == max.pc1 )[1]
max.pc1 = round(max.pc1,4)
```
```{r, echo=TRUE}
plot( results.pca$rotation[,1], results.pca$rotation[,2], pch=16, col= depth.colors, xlab="PC1", ylab="PC2", main="PCA by Sample Depth" )
legend( "topright", c(paste(max.pc1," (Max)"), paste(median.pc1," (Median)"), paste(min.pc1," (Min)")), fill=c(depth.colors[max.index], depth.colors[median.index], depth.colors[min.index]) )
```

Alternatives?
===

- Principle Coordinates Analysis
- Canonical Correspondance Analysis

PCoA: in quick theory
===

PCoA: in practice
===

# Add picture
- The magic is in the metric
- By default you only get 2 dimensions

PCoA: in code (Bray-curtis)
===

```{r, echo=FALSE}
pcoa.data = data
pcoa.data = sweep( pcoa.data, 2, colSums( pcoa.data ), "/")
nmds.b.c.result = metaMDS( comm=t(pcoa.data), distance="bray", k=2, autotransfer=FALSE, trymax=10)
```
```{r}
plot( nmds.b.c.result$points[,1], nmds.b.c.result$points[,2], col=depth.colors, main="Ordination by Bray Curtis")
legend( "topleft", c(paste(max.pc1," (Max)"), paste(median.pc1," (Median)"), paste(min.pc1," (Min)")), fill=c(depth.colors[max.index], depth.colors[median.index], depth.colors[min.index]) )
```

PCoA: in code (Jaccard)
===

```{r, echo=TRUE}
nmds.j.result = metaMDS( comm=t(pcoa.data), distance="jaccard", k=2, autotransfer=FALSE, trymax=10)
plot( nmds.j.result$points[,1], nmds.j.result$points[,2], col=depth.colors, main="Ordination by Jaccard")
legend( "topleft", c(paste(max.pc1," (Max)"), paste(median.pc1," (Median)"), paste(min.pc1," (Min)")), fill=c(depth.colors[max.index], depth.colors[median.index], depth.colors[min.index]) )
```

PCoA: in code (Reciprocal)
===

TODAY

CCA: in quick theory
===

CCA: in practice
===

CCA: in code
===

Next Steps in Ordination
===

Other options to explore
- Sparse PCA

Unsupervised Substructure Discovery
===

Often a goal of scProjects is to describe new structure to a group of cells:
- Heterogeniety of tumor populations
- Novel steps in development
- Robust / dynamic cellular signalling

PCA + Anova: PCA
===

```{r}
results.pca.features <- prcomp( scale( log( t(data) + 1 ), center=TRUE, scale=TRUE), retx=TRUE)
plot( results.pca.features)
```

PCA + Anova: PCA
===

```{r}
hist( abs( results.pca.features$rotation[,1] ), main="Feature Loadings (Abs)", xlab="Feature loadings (Abs)" )
ordered.pca.loadings <- order( results.pca.features$rotation[,1],decreasing = TRUE )
extreme.ends.pca <- c(ordered.pca.loadings[1:250], ordered.pca.loadings[5154:5654])
abline( v=results.pca.features$rotation[,1][extreme.ends.pca], col="#ff000010")
```

PCA + Anova: visualize
===

```{r}
data.scaled.subset = t( scale( t( as.matrix( log( data[ extreme.ends.pca, ] + 1 ) ) ), center=TRUE, scale=TRUE ) )
func_heatmap( data.scaled.subset, str_cor ="euclidean")
```

PCA + Anova: select sample groups
===

```{r}
#dist.col = dist( t( data.scaled.subset ), method="euclidean")
#dendrogram.col = as.dendrogram( hclust( dist.col, method="average" ) )
gmd.dist <- gmdm2dist( gmdm( t( data.scaled.subset ) ) )
gmd.clust <- css.hclust( gmd.dist, hclust( gmd.dist ) )
gmd.elbow.groups <- elbow.batch( gmd.clust, ev.thres=.9, inc.thres=.05 )
```

PCA + Anova: another view
===

```{r}
results.pca.subset = prcomp( data.scaled.subset, retx = TRUE )
plot( results.pca.subset$rotation[,1], results.pca.subset$rotation[,2], pch=16, xlab="PC1", ylab="PC2", main="PCA by Sample Depth", col = depth.colors )
```

PCA + Anova: select feature groups
===

mclust
===

```{r, echo=FALSE}
plot( results.pca$rotation[,1], results.pca$rotation[,2], pch=16 )
mclust.results = Mclust(results.pca$rotation[,c(1:2)])
plot( mclust.results )
```

Compare Methods
===



scOpportunities: cell cycle plot
===

TODAY

scOppotunities: filter by apoptosis
===

TODAY

Statistical Inference in scData
===

- Baysian analysis
- Mixture Models
- Bimodal assumptions

SCDE: in quick theory
===

SCDE: in practice
===

SCDE: in code
===

TODAY

Summary: of the data
===

We are still understanding scData and how to apply it

- Not normal
- Zero-inflated
- Multimodal
- Over-dispersed
...

Summary: of methods
===


Summary: of today
===

- Created expectations on scData
- Explored filtering / normalization techniques
- Applied 3 ordination techniques
- Tried 2 methods to detect substructure
- Applied 1 statistical inference method

Thank you
===

- Aviv Regev
- Alex
- Rahul
- Manik Kuchroo

Questions?
===

# Add picture

Notes: to make a pdf
===

- Create a pdf file before you plot ( can plot multiple plots )
- Close the plotting

```{r}
pdf( "data/my_file.pdf", useDingbats = FALSE ) # Start pdf
plot( 1:10, log(1:10 ) ) # plot in to the pdf file
plot( seq(0,.9,.1), sin(0:9) ) # another plot for the pdf file
dev.off() # Close pdf file ( very important )
```