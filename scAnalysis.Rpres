```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
```

scAnalysis
========================================================
author: Timothy Tickle and Brian Haas
css: 2014_scAnalysis.css
date: September 23, 2014

Logistics
===

```{r, tidy=TRUE}
library(boot) #SCDE
library(caret) #Near-zero filter
library(GMD) #Cluster selection on dendrograms
library(gplots) #Colorpanel
library(mclust) #Selection of clusters
library(scatterplot3d) #3D plotting
library(scde) #Statistical inference (bayesian mixed model)
library(vegan) #PCoA, distance metrics
source("heatmap.3b.R") #Custom HCL
source("cell_cycle_plot.R") #Custom plot for cell cycle
source("Modules.R") #
```

Today's Data Set
===

- Describe data set

How do we get it?
===

```{r}
# Load tab delimited file
data = read.delim( file.path("data","GSE29087_L139_expression_tab.txt"), row.names = 1 )

# For convenience splitting the data frame in to metadata and data
metadata = data[ 1:6 ]
data = data[ -1 * 1:6 ]

# Remove features without counts
zero.features <- which( apply( data, 1, sum ) == 0 )
length( zero.features )
data = data[ -1 * zero.features, ]
metadata = metadata[ -1 * zero.features, ]

# Get groupings
data.groups=c("A01","B01","C01","D01","E01","E01","F01","G01","H01","A02","B02","C02","D02","E02","F02","G02","H02","A03","B03","C03","D03","E03","F03","G03","H03","A04","B04","C04","D04","E04","F04","G04","H04","A05","B05","C05","D05","E05","F05","G05","H05","A06","B06","C06","D06","E06","F06","G06","H06","A07","B07","C07","D07","E07","F07","G07","H07","A08","B08","C08","D08","E08","F08","G08","H08","A09","B09","C09","D09","E09","F09","G09","H09","A10","B10","C10","D10","E10","F10","G10","H10","A11","B11","C11","D11","E11","F11","G11","H11","A12","B12","C12","D12","E12","F12","G12","H12")
```
TODO update

Always look at your data
===

# Add picture

A quick look at the data
===

```{r}
dim( data )
names( data )
summary(data)

dim( metadata )
names( metadata )
summary(metadata)
```

Let's characterize this data.
===

- Is the data normal?
- Sparsity / zero-inflation
- Overdispersed

Normality?
===

```{r}
feature.sum = apply( data, 1, sum )
feature.sum.sorted = sort( feature.sum )
plot( log( feature.sum.sorted ), main = "Total gene counts throughout samples", ylab = "Log total counts", xlab = "Index after sorting" )
abline( v = c(1, 3729,7457,11180,14913), col = c("red", "violet","cyan","blue","green"))
# Min 1, 1st quartile 3729, Median 7457, 3rd quartile 11180, Max 14913
```

Normality?
===

```{r}
feature.sum.order <- order( feature.sum )
index_min <- feature.sum.order[ 1:10 ]
index_q1 <- feature.sum.order[ 3724:3723 ]
index_median <- feature.sum.order[ 7452:7461 ]
index_q3 <- feature.sum.order[ 11175:11184 ]
index_max <- feature.sum.order[ 14908:14913 ]
plot( density( as.matrix( data[ index_min[1], ] ) ), col = "red" )
```

Zooming in to Gene Distributions
===
```{r, echo = FALSE, fig.keep='last'}
plot( x=0,y=0,type="p", xlim=c(0,100), ylim=c(0,.2) )
for( i_min_plot in index_min ){ lines( density( as.matrix( data[ i_min_plot, ])), col = "red", add = TRUE)}
for( i_q1_plot in index_q1 ){ lines( density( as.matrix( data[ i_q1_plot, ])), col = "violet", add = TRUE)}
for( i_median_plot in index_median ){ lines( density( as.matrix( data[ i_median_plot, ])), col = "cyan", add = TRUE)}
for( i_q3_plot in index_q3 ){ lines( density( as.matrix( data[ i_q3_plot, ])), col = "blue", add = TRUE)}
for( i_max_plot in index_max ){ lines( density( as.matrix( data[ i_max_plot, ])), col = "green", add = TRUE)}
```

Sparsity
===

```{r}
# Check samples for read depth
sample.depth <- apply( data, 2, sum )
summary(sample.depth )
```
---
```{r}
barplot( sort( sample.depth ), main = "Sample Depth ", xlab = "Sample", ylab = "Depth")
```

Sparsity
===

```{r}
# Percent zero by feature expression
feature.percent.zero <- apply( data, 1, function(x){ length( which(x==0))/length(x)})
feature.mean.no.zero <- apply( data, 1, function(x){ mean( x[ which(x!=0) ] ) })
plot( feature.percent.zero, log( feature.mean.no.zero ), xlab = "log mean", ylab = "Percent Zero", main = "Feature Sparsity by Expression" )
```

Overdispersion
===
```{r}
# SD vs Mean
feature.sd.with.zero <- apply( data, 1, sd )
feature.mean.with.zero <- apply( data, 1, mean )
feature.sd.no.zero <- apply( data, 1, function(x){ sd( x[ which(x !=0 )])})
plot( log( feature.mean.no.zero ), log( feature.sd.no.zero ), xlab = "Mean (Log)", ylab = "SD (Log)", main = "SD vs mean (ignoring zeros)" )
plot( log( feature.mean.with.zero ), log( feature.sd.with.zero ), xlab = "Mean (Log)", ylab = "SD (Log)", main = "SD vs mean (ignoring zeros)" )
```

Can QC Help?
===

- Removing very sparse features
- Imputing outliers

Removing Sparse Features
===

```{r}
sample.percentile = apply( data, 2, function(x){ quantile(x[x !=0 ], .5)})
feature.noise = which(apply( data, 1, min_occurence_at_min_value, sample.percentile ) <= 10)
feature.noise.by.expression = order( apply( data[ feature.noise, ], 1, sum ), decreasing = TRUE)
# What am I removing
plot(density( as.matrix(data[ feature.noise[ feature.noise.by.expression[ 1 ]], ])))
# TODO make a multiple density plot
```

Removing Sparse Features
===

```{r}
dim( data )
dim( metadata )
data = data[ -1 * feature.noise, ]

metadata = metadata[ -1 * feature.noise ]
dim( data )
dim( metadata )

data.groups <- data.groups[ -1 * feature.noise ]
```

Sample Read Depth: revisited
===

```{r}
sample.depth <- apply( data, 2, sum )
depth.colors = polychromatic_palette( length( sample.depth ) )
sample.depth
```
---
```{r}
barplot( sort(apply( data, 2, sum )))
```

Can Transforms / Normalization Help?
===

NEED CODE

Challenge
===

- We have a large range of read depth per sample
  - Is this a problem?

Waste Not, Want Not
===

- Rarefy?

Dimensionality Reduction and Ordination
===

- A frequently used method is PCA

PCA: in quick theory
===

- Describe PCA

PCA: in practice
===

- Things to be aware of

PCA: in code
===

```{r, echo=TRUE}
# Row center and log
data.scaled = t( scale( t( as.matrix( log( data + 1 ) ) ), center=TRUE, scale=TRUE ) )
# Remove constant rows
# TODO data.scaled = data.scaled[, -1 * nearZeroVar( data.scaled ) ]
# Perfrom PCA
results.pca = prcomp( data.scaled, retx = TRUE )
```

PCA: in code
===

```{r, echo=TRUE}
plot( results.pca$rotation[,1], results.pca$rotation[,2], pch=16 )
```

PCA: by depth
===
```{r,echo=FALSE}
min.pc1 = min( results.pca$rotation[,1] )
min.index = which(sort(results.pca$rotation[,1]) == min.pc1 )[1]
min.pc1 = round(min.pc1,4)
median.pc1 = median(c(0,results.pca$rotation[,1]))
median.index = which(sort(results.pca$rotation[,1]) == median.pc1 )[1]
median.pc1 = round(median.pc1,4)
max.pc1 = max( results.pca$rotation[,1] )
max.index = which(sort(results.pca$rotation[,1]) == max.pc1 )[1]
max.pc1 = round(max.pc1,4)
```
```{r, echo=TRUE}
plot( results.pca$rotation[,1], results.pca$rotation[,2], pch=16, col= depth.colors, xlab="PC1", ylab="PC2", main="PCA by Sample Depth" )
legend( "topright", c(paste(max.pc1," (Max)"), paste(median.pc1," (Median)"), paste(min.pc1," (Min)")), fill=c(depth.colors[max.index], depth.colors[median.index], depth.colors[min.index]) )
```

PCA: 3D with caution
===

```{r}
scatterplot3d( x=results.pca$rotation[,1], y=results.pca$rotation[,3], z=results.pca$rotation[,2], color = depth.colors, xlab="PC1", ylab="PC3", zlab="PC2", main="3D PCA using Components 1-3" )
```

Alternatives?
===

- Principle Coordinates Analysis
- Wieghted PCA
  - Different than Sparse PCA

PCoA: in quick theory
===

PCoA: in practice
===

#? Add picture
- The magic is in the metric
- By default you only get 2 dimensions

PCoA: in code (Bray-curtis)
===

```{r, echo=FALSE}
pcoa.data = data
pcoa.data = sweep( pcoa.data, 2, colSums( pcoa.data ), "/")
nmds.b.c.result = metaMDS( comm=t(pcoa.data), distance="bray", k=2, autotransfer=FALSE, trymax=10)
```
```{r}
plot( nmds.b.c.result$points[,1], nmds.b.c.result$points[,2], col=depth.colors, main="Ordination by Bray Curtis")
legend( "topleft", c(paste(max.pc1," (Max)"), paste(median.pc1," (Median)"), paste(min.pc1," (Min)")), fill=c(depth.colors[max.index], depth.colors[median.index], depth.colors[min.index]) )
```

PCoA: in code (Jaccard)
===

```{r, echo=TRUE}
nmds.j.result = metaMDS( comm=t(pcoa.data), distance="jaccard", k=2, autotransfer=FALSE, trymax=10)
plot( nmds.j.result$points[,1], nmds.j.result$points[,2], col=depth.colors, main="Ordination by Jaccard")
legend( "topleft", c(paste(max.pc1," (Max)"), paste(median.pc1," (Median)"), paste(min.pc1," (Min)")), fill=c(depth.colors[max.index], depth.colors[median.index], depth.colors[min.index]) )
```

Next Steps in Ordination
===

Other options to explore
- Wieghted PCA

Unsupervised Substructure Discovery
===

Often a goal of scProjects is to describe new structure to a group of cells:
- Heterogeniety of tumor populations
- Novel steps in development
- Robust / dynamic cellular signalling

PCA + ANOVA: PCA
===

```{r}
results.pca.features <- prcomp( scale( log( t(data) + 1 ), center=TRUE, scale=TRUE), retx=TRUE)
plot( results.pca.features)
```

PCA + ANOVA: PCA
===

```{r}
hist( abs( results.pca.features$rotation[,1] ), main="Feature Loadings (Abs)", xlab="Feature loadings (Abs)" )
ordered.pca.loadings <- order( results.pca.features$rotation[,1],decreasing = TRUE )
extreme.ends.pca <- c(ordered.pca.loadings[1:250], ordered.pca.loadings[5154:5654])
abline( v=results.pca.features$rotation[,1][extreme.ends.pca], col="#ff000010")
```

PCA + ANOVA: visualize
===

```{r}
data.scaled.subset = t( scale( t( as.matrix( log( data[ extreme.ends.pca, ] + 1 ) ) ), center=TRUE, scale=TRUE ) )
heatmap( data.scaled.subset )
```

PCA + Anova: select sample groups
===

```{r}
gmd.dist <- gmdm2dist( gmdm( t( data.scaled.subset ) ) )
hclust.aov <- hclust( gmd.dist )
gmd.clust <- css.hclust( gmd.dist, hclust.aov )
gmd.elbow.groups <- elbow.batch( gmd.clust, ev.thres=0.9, inc.thres=.05 )
tree.groups <- as.factor( cutree( hclust.aov, k=gmd.elbow.groups$k ) )
```

PCA + ANOVA: select sample groups
===

```{r}
heatmap( data.scaled.subset, vctr_grouping=tree.groups )
```

PCA + ANOVA: Another view
===

```{r}
results.pca.subset = prcomp( data.scaled.subset, retx = TRUE )
plot( results.pca.subset$rotation[,1], results.pca.subset$rotation[,2], pch=16, xlab="PC1", ylab="PC2", main="PCA by Sample Depth", col = depth.colors )
```

PCA + ANOVA: Another view
===

```{r}
tree.group.colors = func_factor_to_metadata_color( tree.groups )$vctr_grouping_colors
plot( results.pca.subset$rotation[,1], results.pca.subset$rotation[,2], pch=16, xlab="PC1", ylab="PC2", main="PCA by Grouping (Dendrogram)", col = tree.group.colors )
```

PCA + ANOVA: select feature groups
===

NEED CODE
data.scaled.subset

mclust
===

```{r, echo=FALSE}
plot( results.pca$rotation[,1], results.pca$rotation[,2], pch=16 )
mclust.results = Mclust(results.pca$rotation[,c(1:2)])
plot( mclust.results )
```

Compare Methods
===

NEED CODE

scOpportunities: cell cycle plot
===

NEED CODE

Statistical Inference in scData
===

- Bayesian analysis
- Mixture Models
- Bimodal assumptions

SCDE: in quick theory
===

SCDE: in practice
===

SCDE: in code
===

NEED CODE

scde.error.models( as.matrix( data ), groups = groups.discovered, n.cores=1 )

PCoA: in code (Reciprocal)
===

NEED CODE
```{r}
dist.recip <- reciprocal_weighting( x=as.matrix(data), groups=data.groups )
nmds.r.result = metaMDS( comm=dist.recip, k=2, autotransfer=FALSE, trymax=10)
plot( nmds.r.result$points[,1], nmds.r.result$points[,2], col=depth.colors, main="Ordination by Reciprocal Weighting")
```

What did we find ?
===

- We discovered groups of cells
- We found significantly associated features
- How do we connect this to biology?

Gene Set Enrichment Analysis: GSEA
===

- Not specifically related to scAnalysis
- Does help round out the story
- Many options
  - DAVID (online or R library RDAVIDWebService)
  - GSEA (online or many libraries)
    - wilcoxGST from the limma library
    - GSEABase
    
GSEA: briefly described
===

What did we find ???
===


Summary: of the data
===

- We are still understanding scData and how to apply it
  - Not normal
  - Zero-inflated
  - Multimodal
  - Over-dispersed
- Keeping these characteristics in analysis assumptions

Summary: of methods
===


Summary: of today
===

- Created expectations on scData
- Explored filtering / normalization techniques
- Applied 3 ordination techniques
- Tried 2 methods to detect substructure
- Applied 1 statistical inference method

Thank you
===

- Aviv Regev
- Alex Shalek
- Manik Kuchroo
- Rahul Satija
- Itay Tirosh

Questions?
===

# Add picture

Notes: to make a pdf
===

- Create a pdf file before you plot ( can plot multiple plots )
- Close the plotting

```{r}
pdf( "data/my_file.pdf", useDingbats = FALSE ) # Start pdf
plot( 1:10, log(1:10 ) ) # plot in to the pdf file
plot( seq(0,.9,.1), sin(0:9) ) # another plot for the pdf file
dev.off() # Close pdf file ( very important )
```